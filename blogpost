Blog Post - Seminar Computer Vision by Deep Learning
Brain Tumor Segmentation Model Performance Analysis
Authors: Tessel Huibregtsen (4876377), Jonah Pedra (5003768), Olivier Heukelom (5009448)

In this blogpost, we present the motivation, methods and results for our performance analysis of a pre-trained brain tumor detection model. This work is part of the CS4245 Seminar Computer Vision by Deep Learning Course 2023/2024 at TU Delft. Our source code and augmented dataset can be accessed through https://github.com/tesselhuib/tumor_segmentation_model_sensitivity.git. 

Introduction
Magnetic Resonance Imaging (MRI) is a crucial tool in medical diagnostics of the brain. They are particularly useful to detect brain tumors in their early stages, or to determine the size and shape of the tumor in an advanced stage [1]. Specialized segmentation models have the potential to significantly enhance the accuracy and efficiency of the detection of brain tumors, providing valuable support to radiologists. However, these models are typically trained on clean, high quality data [2]. In practice however, a number of factors can result in suboptimal MRI scans, which may decrease the performance of these segmentation models [3]. This creates an interesting opportunity to characterize the accuracy of these models on affected scans. How much augmentation can be applied before these models fail by a significant amount? Within this project, the performance of a pre-trained segmentation model on MRI scans with realistic imperfections will be evaluated. 
Research Question
How do simulated artifacts in MRI images related to motion artifacts affect the performance of brain tumor detection by a pre-trained segmentation model?
Motivation
The image resolution of an MRI machine is largely determined by their magnetic field strength, where machines with a higher Tesla (3T and above) provide better image resolution but are more expensive, often costing over $1 million [4]. The high cost of these machines limits access to state-of-the-art diagnostic ability in disadvantaged regions. Another challenge are shortened scan times due to time constraints or patient inability to remain still for extended periods. 
In addition, the accuracy and reliability of MRI scans can be compromised by the presence of a variety of artifacts. These include hardware, sequence (the specific protocol used to generate images), and patient-related artifacts. 
A robust segmentation model that generalizes well to artifacts and lower quality scans would allow the use of less powerful MRI machines and shorter scan times without compromising diagnostic accuracy, leading to substantial cost reductions and increased patient throughput. 
Augmentation Method

This exploration will focus on patient-related motion artifacts, and specifically characterize how much motion can be captured within a scan before there are detrimental effects on the detection ability of a tumor segmentation model. 
In this exploration, we will use the following image augmentation techniques to simulate the diagnostic challenges within MRI scans of the brain using computer vision which exacerbate the risk of incorrect interpretations and adversely affect patient care and treatment plans. 
Motion artifacts will be introduced through both rotation and translation to simulate movement of a patient during scanning. In order to simulate these kinds of motion artifacts, the images shall be manipulated in the fourier space. First, the images shall be translated into the fourier space, prior to centering the lowest frequencies present in the image. Thereafter, a mask shall be applied to the fourier transformed image, prior to a reverse fourier transform to achieve the final image. The size of the mask applied corresponds to the intensity of the alteration, whilst the shape corresponds to the direction in which the translation is simulated. A smaller mask will in turn result in a larger alteration to the image, whilst on the other hand if the shape is concentrated around one specific axis, this will result in a corresponding motion artifact in the opposing axis, assuming it is a 2D slice being investigated. In order to simulate the level of augmentation that would typically be present in MRI scans, a mask with largest dimensions corresponding to ~90% of image shape should be used, however this value shall be iterated upon throughout the investigation [9]. An example of this is visible below in Figure 1, where a minor horizontal translation motion artifact is introduced in the fourier space of a t1ce scan, with the specific mask of the magnitude spectrum visible in the top figures, whilst the input and output images are visible in the lower two figures.

Fig. 1. Sample augmentation for T1CE space MRI scan. Input image and magnitude spectrum are visible on the left side, whilst the masked version and output image are visible on the bottom right.
Introduction to the Segmentation Model
For this project, we made use of the pretrained model from https://github.com/GBM-images-research/SegResNet_GBMsegmentation. This model was trained on the BraTS-2018 dataset. BraTS is a yearly brain segmentation challenge where each year focuses on a specific application [5]. In 2018, the BraTS dataset consisted of preoperative scans of patients with glioblastoma, a type of brain tumor. For each patient, four modality scans were acquired: T1, T2, T1 contrast-enhanced, and flair. We will not go into too much detail of each modality in this blog post as that is out of the scope of the project, but to put it short: each modality focuses on a different physical reaction of the brain to the magnetic field imposed by the MRI. Therefore, each modality gives you extra information about the composition of the brain. More information can be found here [6]. The goal of the challenge was to distinguish three different parts of the tumor: the necrotic and non-enhancing tumor core (NET, label 1 in the ground truth), the peritumoral edema (ED, label 2 in the ground truth) and the enhancing tumor (ET, label 3 in the ground truth). In Figure 2 an example of the input data is visualized.

Fig. 2. Example of the input data. The same z-slice is presented for each of the four modalities and the ground truth for the same patient. The ground truth contains the 3 tumor labels, as annotated by experts. Dark gray (the innermost part) is the NET, white (the ring around the innermost part) the ET and light gray (the outsides) is the ED.
The pretrained model is based on the SegResNet network by Monai [7]. This network is based on the paper by Myronenko in 2018 [8], which describes the network that won the BraTS-2018 challenge, but SegResNet does not include the variational autoencoder. The network consists of an initial 3D convolution with 32 filters and is followed by an encoder and decoder part. Both the encoder and decoder consist of sequential ResNet blocks using a GroupNorm normalization. A schematic overview of the network can be seen in Figure 3. 

Fig. 3. The network architecture that SegResNet is based on. The right bottom part is the variational autoencoder, which is not included in SegResNet. The input size of the network is the 4 modalities x image dimensions and the output size is the 3 labels x image dimensions. Figure adjusted from [8]. 
Adversarial attack
With the pretrained model and the augmented BraTS-2018 database, we set up a pipeline to perform the adversarial attacks. The input images were rescaled to 160x240x240 to be able to pass through the network. Next to that, the images were normalized using the image mean and standard deviation. After the inference on the model was done, we plotted the result using a histogram to evaluate the results. As visible in Figure 4, there is a visible split between two classes (background and foreground) for each label. To be able to create a mask from this data, we applied a sigmoid and defined a threshold. The optimal threshold for each label was determined based on trial and error. Next, we ran both the unaugmented and increased levels of augmentations through the model. We applied the sigmoid and the threshold and consequently calculated the dice score for each individual brain. In the next section, these results will be explained.

Fig. 4. Example histograms of the output of an unaugmented image passed through the model. For each label, a split between background (the high peak on the left) and foreground (smaller peak on the right) can be seen. However, these peaks occur at different places and therefore different thresholds were used for each label.
Results
After calculating the dice score for each augmentation level for 10 patients passed through the model, we plotted the average dice scores of these patients with respect to this augmentation level. The results are shown in Figures 5. As the pretrained model received poor segmentation results for label 1 and 2 (original dice scores of 0.17 and 0.07 respectively), we only used label 3. 

Fig. 5. The average dice scores of 10 patients plotted against the augmentation level, where 1 is no augmentation (the original image) and decreasing values mean increasing augmentation. Only the results for level 3 are shown as the pre-trained model performed segmentation best on this label. (B) is a zoomed in version of (A) to better visualize the decrease in accuracy. 
There are multiple interesting things to note. First of all, what we would expect: with decreasing mask size (and thus increased augmentation) the model performs worse at segmenting the tumors. However, we seem to lose some crucial information in performing any augmentation at all, as the largest decrease in accuracy happens between the original and a mask of 0.99. We are unsure what exactly the cause is for this, but hypothesize that the high frequency part of the fourier spectrum contains crucial information for the model. An interesting next step for research would be to reverse the mask, so remove the low frequencies first, and see how the model reacts. Another possible explanation would be that in our augmentation pipeline, something in the images is altered which we are unaware of. Potentially in the initial transformation or in the saving to a .nii file, something changes that affects the network’s ability to segment the tumor. This is something that should be explored further.

We also explored this drop in accuracy by trying to see where the model starts making mistakes in its segmentations. In Figure 6, we can see that around the tumor it actually does quite well in segmenting the tumor, almost matching the segmentations of the unaugmented images. This is seen across various patients, but not all. However, in the augmented version, large other parts of the brain, in different slices, seem to be segmented as well. We are unsure what is causing this. 


Fig. 6. Segmentation mask example of an augmented brain with a 0.99 mask compared to its unaugmented counterpart and the ground truth. Only Label 3 (ET) is shown. The rows show different slices in the brain. As visible in the ground truth, the tumor exists in slice 50 & 80 but not in slice 100. The augmented image has very similar segmentations when the tumor is present, but also segments large parts of the brain in a slice where the tumor is not present. 
As a note, we would also like to mention that the base performance of the algorithm was already quite low. Dice scores of around 0.3 are not considered accurate in the medical field. Next to this, the variability within the scores of the 10 patients was quite high. For some patients it segmented very well, whereas for others it didn’t at all. It would have been worth doing all these tests with more patients, but Kaggle memory limits did not allow us to do this. Therefore, it could be both possible that our results are influenced by the fact the model was not yet trained well enough and by the fact that we only tested for 10 different patients.

Conclusion

In conclusion, motion artifacts with both rotation and translation were used to simulate movement of a patient during scanning. In total MRI scans of 10 patients were used with a masked Fourier space. When passed through the SegResNet network by Monai [7] it was discovered that with decreasing mask size (and thus increased augmentation) the model performs worse at segmenting the tumors, as expected. In addition, the dice scores showed that relatively, label 3 was the most robust of the model. However, since the largest drop-off in average dice score happens right at the beginning, we hypothesize that specifically the high frequency part of the fourier spectrum contains crucial information for the model. 

Recommendations

As observed, most of the loss in accuracy seems to stem from the removal of the high frequency domain of the Fourier spectrum. As such, it would be interesting to see how the model would react to images with the low frequency domain removed instead.

In addition, the variability within the dice scores of images from the 10 patients used in this study was quite high. Therefore, we recommend using a system with more memory to give more insightful results. 



References: 

American Health Imaging. (2023, November 1). How is an MRI used to detect brain tumors? American Health Imaging. https://americanhealthimaging.com/how-is-an-mri-used-to-detect-brain-tumors/
Liu, Z., Tong, L., Chen, L., Jiang, Z., Zhou, F., Zhang, Q., Zhang, X., Jin, Y., & Zhou, H. (2023). Deep learning based brain tumor segmentation: A survey. Complex & Intelligent Systems, 9(1), 1001–1026. https://doi.org/10.1007/s40747-022-00815-5
Heiland, S. (2008). From A as in Aliasing to Z as in Zipper: Artifacts in MRI. Clinical Neuroradiology, 18(1), 25–36. https://doi.org/10.1007/s00062-008-8003-y
Arnold, T. C., Freeman, C. W., Litt, B., & Stein, J. M. (2023). Low-field MRI: Clinical promise and challenges. Journal of Magnetic Resonance Imaging, 57(1), 25–44. https://doi.org/10.1002/jmri.28408 
Menze, B. H., Jakab, A., Bauer, S., Kalpathy-Cramer, J., Farahani, K., Kirby, J., Burren, Y., Porz, N., Slotboom, J., Wiest, R., Lanczi, L., Gerstner, E., Weber, M.-A., Arbel, T., Avants, B. B., Ayache, N., Buendia, P., Collins, D. L., Cordier, N., … Van Leemput, K. (2015). The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS). IEEE Transactions on Medical Imaging, 34(10), 1993–2024. https://doi.org/10.1109/TMI.2014.2377694
MRI  Basics. (n.d.). Retrieved 16 June 2024, from https://case.edu/med/neurology/NR/MRI%20Basics.htm
SegResNet, MONAI : https://docs.monai.io/en/stable/networks.html#segresnet 
Myronenko, A. (2018). 3D MRI brain tumor segmentation using autoencoder regularization (arXiv:1810.11654). arXiv. http://arxiv.org/abs/1810.11654
Wang NC, Noll DC, Srinivasan A, Gagnon-Bartsch J, Kim MM, Rao A. Simulated MRI Artifacts: Testing Machine Learning Failure Modes. BME Front. 2022 Nov 1;2022:9807590. doi: 10.34133/2022/9807590. PMID: 37850164; PMCID: PMC10521705.


